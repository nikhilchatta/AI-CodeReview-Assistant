name: AI Code Review Gate

on:
  workflow_call:
    inputs:
      max_files:
        description: Maximum number of changed files to send for review
        type: number
        default: 20
      file_extensions:
        description: Comma-separated file extensions to include (e.g. .py,.ts,.scala)
        type: string
        default: .py,.ts,.tsx,.js,.jsx,.scala,.sql,.tf
      timeout_minutes:
        description: Maximum minutes the review job is allowed to run
        type: number
        default: 30
    secrets:
      AI_AGENT_API_URL:
        required: true
        description: Base URL of the AWS-hosted AI agent backend (e.g. https://review.example.com)
      AI_AGENT_API_KEY:
        required: true
        description: API key for authenticating with the AI agent backend

jobs:
  ai-code-review-gate:
    name: AI Code Review Gate
    runs-on: ubuntu-latest
    timeout-minutes: ${{ inputs.timeout_minutes }}
    permissions:
      contents: read
      pull-requests: write

    steps:
      # â”€â”€ Step 1: Checkout PR code with full history for diff â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Checkout PR code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          # Explicitly check out the PR head so HEAD points to the PR branch,
          # not the base branch. Required for pull_request_target triggers.
          ref: ${{ github.event.pull_request.head.sha }}

      # â”€â”€ Step 2: Compute changed files from PR diff â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Compute changed files from PR diff
        id: diff
        env:
          BASE_REF: ${{ github.base_ref }}
          PR_HEAD_SHA: ${{ github.event.pull_request.head.sha }}
          MAX_FILES: ${{ inputs.max_files }}
          EXTENSIONS: ${{ inputs.file_extensions }}
        run: |
          # fetch-depth: 0 already pulled full history; just ensure origin/BASE_REF
          # ref is up to date without making it shallow (--depth=1 would break
          # the common-ancestor calculation in the 3-dot diff below).
          git fetch origin "${BASE_REF}"

          # Use the explicit PR head SHA so this works for both pull_request
          # and pull_request_target triggers (for the latter, HEAD is the
          # base branch and would produce an empty diff without this fix).
          DIFF_HEAD="${PR_HEAD_SHA:-HEAD}"
          echo "Diffing: origin/${BASE_REF}...${DIFF_HEAD}"
          echo "HEAD commit: $(git rev-parse HEAD)"
          echo "PR head SHA: ${DIFF_HEAD}"

          # All changed files (no extension filter) for debug visibility
          ALL_FILES=$(git diff --name-only --diff-filter=ACMRT \
            "origin/${BASE_REF}...${DIFF_HEAD}" 2>/dev/null || true)
          echo "All changed files (unfiltered):"
          echo "${ALL_FILES:-  (none)}"

          PATTERNS=$(echo "$EXTENSIONS" | tr ',' '\n' | sed 's/^\./\*./g' | tr '\n' ' ')
          echo "Extension patterns: ${PATTERNS}"

          FILES=$(git diff --name-only --diff-filter=ACMRT \
            "origin/${BASE_REF}...${DIFF_HEAD}" \
            -- $PATTERNS 2>/dev/null | head -n "$MAX_FILES")

          if [ -z "$FILES" ]; then
            echo "no_files=true"  >> "$GITHUB_OUTPUT"
            echo "No relevant files changed in this PR. Skipping AI review."
          else
            echo "no_files=false" >> "$GITHUB_OUTPUT"
            echo "$FILES" > /tmp/changed_files.txt
            FILE_COUNT=$(echo "$FILES" | wc -l | tr -d ' ')
            echo "file_count=${FILE_COUNT}" >> "$GITHUB_OUTPUT"
            echo "Found ${FILE_COUNT} file(s) to review:"
            cat /tmp/changed_files.txt
          fi

      # â”€â”€ Step 3: Early exit when no reviewable files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Skip â€” no reviewable files changed
        if: steps.diff.outputs.no_files == 'true'
        run: |
          echo "No files matching [ ${{ inputs.file_extensions }} ] were changed."
          echo "AI code review gate skipped â€” nothing to review."

      # â”€â”€ Step 4: Build JSON payload from changed files â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Build review payload
        id: payload
        if: steps.diff.outputs.no_files == 'false'
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_HEAD_REF: ${{ github.head_ref }}
          GITHUB_BASE_REF: ${{ github.base_ref }}
          GITHUB_SHA: ${{ github.sha }}
          GITHUB_ACTOR: ${{ github.actor }}
          GITHUB_RUN_ID: ${{ github.run_id }}
        run: |
          python3 << 'PYEOF'
          import json, os, sys

          EXT_LANG = {
            '.py':    'python',
            '.pyspark': 'pyspark',
            '.ts':    'typescript',
            '.tsx':   'typescript',
            '.js':    'javascript',
            '.jsx':   'javascript',
            '.scala': 'scala',
            '.sql':   'sql',
            '.tf':    'terraform',
          }

          workspace = os.environ.get('GITHUB_WORKSPACE', '')

          files = []
          with open('/tmp/changed_files.txt') as fh:
            paths = [line.strip() for line in fh if line.strip()]

          for fpath in paths:
            full_path = os.path.join(workspace, fpath) if workspace else fpath
            if not os.path.isfile(full_path):
              print(f"Skip (deleted/missing): {fpath} (resolved: {full_path})", file=sys.stderr)
              continue
            ext = os.path.splitext(fpath)[1].lower()
            language = EXT_LANG.get(ext)
            if not language:
              continue
            try:
              with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
              if len(content) > 102400:
                content = content[:102400]
                print(f"Truncated {fpath} to 100 KB", file=sys.stderr)
              files.append({'path': fpath, 'content': content, 'language': language})
            except Exception as e:
              print(f"Warning â€” could not read {fpath}: {e}", file=sys.stderr)

          if not files:
            print("No readable files found after filtering.", file=sys.stderr)
            with open(os.environ['GITHUB_OUTPUT'], 'a') as gf:
              gf.write("no_files=true\n")
            sys.exit(0)

          payload = {
            'files': files,
            'metadata': {
              'repository':      os.environ.get('GITHUB_REPOSITORY', ''),
              'pr_number':       int(os.environ.get('PR_NUMBER') or 0),
              'branch':          os.environ.get('GITHUB_HEAD_REF', ''),
              'base_branch':     os.environ.get('GITHUB_BASE_REF', ''),
              'commit_sha':      os.environ.get('GITHUB_SHA', ''),
              'actor':           os.environ.get('GITHUB_ACTOR', ''),
              'workflow_run_id': os.environ.get('GITHUB_RUN_ID', ''),
              'project_id':      os.environ.get('GITHUB_REPOSITORY', '').split('/')[-1],
            }
          }

          with open('/tmp/review_payload.json', 'w') as f:
            json.dump(payload, f)

          import os as _os
          _size = _os.path.getsize('/tmp/review_payload.json')
          print(f"Payload ready: {len(files)} file(s), {_size} bytes written to /tmp/review_payload.json", file=sys.stderr)
          PYEOF

      # â”€â”€ Validate secrets â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Validate secrets
        run: |
          if [ -z "${{ secrets.AI_AGENT_API_URL }}" ] || [ -z "${{ secrets.AI_AGENT_API_KEY }}" ]; then
            echo "::error::Secrets not injected into reusable workflow"
            exit 1
          fi

      # â”€â”€ Step 5: Code complexity analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Code complexity analysis
        id: complexity
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
        run: |
          EMPTY='{"gate_status":"pass","status":"success","total_issues":0,"severity_breakdown":{"critical":0,"high":0,"medium":0,"low":0,"info":0},"validation_failures":[],"llm_findings":[],"per_file_results":[],"token_usage":{"input_tokens":0,"output_tokens":0,"total_tokens":0},"cost_usd":0,"latency_ms":0}'

          if [ ! -f /tmp/review_payload.json ]; then
            echo "No payload â€” skipping complexity analysis."
            echo "$EMPTY" > /tmp/response_complexity.json
            exit 0
          fi

          jq '. + {"analysis_type": "complexity"}' /tmp/review_payload.json > /tmp/payload_complexity.json
          echo "DEBUG payload size: $(wc -c < /tmp/payload_complexity.json) bytes"
          echo "DEBUG files count: $(jq '.files | length' /tmp/payload_complexity.json 2>&1)"
          echo "DEBUG first file: $(jq -r '.files[0].path // "none"' /tmp/payload_complexity.json 2>&1)"
          MAX_RETRIES=3; RETRY_DELAY=10; ATTEMPT=0; SUCCESS=false

          while [ $ATTEMPT -lt $MAX_RETRIES ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo "::group::Complexity analysis â€” attempt ${ATTEMPT}/${MAX_RETRIES}"
            HTTP_STATUS=$(curl -s --max-time 300 --retry 0 \
              -o /tmp/response_complexity.json -w "%{http_code}" \
              -X POST "${API_URL}/api/review/pr" \
              -H "Content-Type: application/json" \
              -H "x-api-key: ${API_KEY}" \
              -d @/tmp/payload_complexity.json)
            echo "HTTP status: ${HTTP_STATUS}"
            echo "::endgroup::"
            if [ "$HTTP_STATUS" = "200" ]; then
              SUCCESS=true; break
            elif [ "$HTTP_STATUS" = "429" ] || [ "$HTTP_STATUS" -ge "500" ] 2>/dev/null; then
              echo "Transient error (HTTP ${HTTP_STATUS}). Waiting ${RETRY_DELAY}s before retry..."
              sleep "$RETRY_DELAY"; RETRY_DELAY=$((RETRY_DELAY * 2))
            else
              echo "::error::Complexity analysis API error: HTTP ${HTTP_STATUS}"
              cat /tmp/response_complexity.json; exit 1
            fi
          done

          if [ "$SUCCESS" != "true" ]; then
            echo "::error::Complexity analysis failed after ${MAX_RETRIES} attempts."
            exit 1
          fi

          echo "Complexity analysis complete."
          jq '{total_issues: .total_issues, severity: .severity_breakdown}' /tmp/response_complexity.json 2>/dev/null || true

      # â”€â”€ Step 6: AI security scan â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: AI security scan
        id: security
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
        run: |
          EMPTY='{"gate_status":"pass","status":"success","total_issues":0,"severity_breakdown":{"critical":0,"high":0,"medium":0,"low":0,"info":0},"validation_failures":[],"llm_findings":[],"per_file_results":[],"token_usage":{"input_tokens":0,"output_tokens":0,"total_tokens":0},"cost_usd":0,"latency_ms":0}'

          if [ ! -f /tmp/review_payload.json ]; then
            echo "No payload â€” skipping security scan."
            echo "$EMPTY" > /tmp/response_security.json
            exit 0
          fi

          jq '. + {"analysis_type": "security"}' /tmp/review_payload.json > /tmp/payload_security.json
          MAX_RETRIES=3; RETRY_DELAY=10; ATTEMPT=0; SUCCESS=false

          while [ $ATTEMPT -lt $MAX_RETRIES ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo "::group::Security scan â€” attempt ${ATTEMPT}/${MAX_RETRIES}"
            HTTP_STATUS=$(curl -s --max-time 300 --retry 0 \
              -o /tmp/response_security.json -w "%{http_code}" \
              -X POST "${API_URL}/api/review/pr" \
              -H "Content-Type: application/json" \
              -H "x-api-key: ${API_KEY}" \
              -d @/tmp/payload_security.json)
            echo "HTTP status: ${HTTP_STATUS}"
            echo "::endgroup::"
            if [ "$HTTP_STATUS" = "200" ]; then
              SUCCESS=true; break
            elif [ "$HTTP_STATUS" = "429" ] || [ "$HTTP_STATUS" -ge "500" ] 2>/dev/null; then
              echo "Transient error (HTTP ${HTTP_STATUS}). Waiting ${RETRY_DELAY}s before retry..."
              sleep "$RETRY_DELAY"; RETRY_DELAY=$((RETRY_DELAY * 2))
            else
              echo "::error::Security scan API error: HTTP ${HTTP_STATUS}"
              cat /tmp/response_security.json; exit 1
            fi
          done

          if [ "$SUCCESS" != "true" ]; then
            echo "::error::Security scan failed after ${MAX_RETRIES} attempts."
            exit 1
          fi

          echo "Security scan complete."
          jq '{total_issues: .total_issues, severity: .severity_breakdown}' /tmp/response_security.json 2>/dev/null || true

      # â”€â”€ Step 7: AI code review â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: AI code review
        id: review
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
        run: |
          EMPTY='{"gate_status":"pass","status":"success","total_issues":0,"severity_breakdown":{"critical":0,"high":0,"medium":0,"low":0,"info":0},"validation_failures":[],"llm_findings":[],"per_file_results":[],"token_usage":{"input_tokens":0,"output_tokens":0,"total_tokens":0},"cost_usd":0,"latency_ms":0}'

          if [ ! -f /tmp/review_payload.json ]; then
            echo "No payload â€” skipping code review."
            echo "$EMPTY" > /tmp/response_review.json
            exit 0
          fi

          jq '. + {"analysis_type": "review"}' /tmp/review_payload.json > /tmp/payload_review.json
          MAX_RETRIES=3; RETRY_DELAY=10; ATTEMPT=0; SUCCESS=false

          while [ $ATTEMPT -lt $MAX_RETRIES ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo "::group::Code review â€” attempt ${ATTEMPT}/${MAX_RETRIES}"
            HTTP_STATUS=$(curl -s --max-time 300 --retry 0 \
              -o /tmp/response_review.json -w "%{http_code}" \
              -X POST "${API_URL}/api/review/pr" \
              -H "Content-Type: application/json" \
              -H "x-api-key: ${API_KEY}" \
              -d @/tmp/payload_review.json)
            echo "HTTP status: ${HTTP_STATUS}"
            echo "::endgroup::"
            if [ "$HTTP_STATUS" = "200" ]; then
              SUCCESS=true; break
            elif [ "$HTTP_STATUS" = "429" ] || [ "$HTTP_STATUS" -ge "500" ] 2>/dev/null; then
              echo "Transient error (HTTP ${HTTP_STATUS}). Waiting ${RETRY_DELAY}s before retry..."
              sleep "$RETRY_DELAY"; RETRY_DELAY=$((RETRY_DELAY * 2))
            else
              echo "::error::Code review API error: HTTP ${HTTP_STATUS}"
              cat /tmp/response_review.json; exit 1
            fi
          done

          if [ "$SUCCESS" != "true" ]; then
            echo "::error::Code review failed after ${MAX_RETRIES} attempts."
            exit 1
          fi

          echo "Code review complete."
          jq '{total_issues: .total_issues, severity: .severity_breakdown}' /tmp/response_review.json 2>/dev/null || true

      # â”€â”€ Step 8: AI code quality analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: AI code quality analysis
        id: quality_scan
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
        run: |
          EMPTY='{"gate_status":"pass","status":"success","total_issues":0,"severity_breakdown":{"critical":0,"high":0,"medium":0,"low":0,"info":0},"validation_failures":[],"llm_findings":[],"per_file_results":[],"token_usage":{"input_tokens":0,"output_tokens":0,"total_tokens":0},"cost_usd":0,"latency_ms":0}'

          if [ ! -f /tmp/review_payload.json ]; then
            echo "No payload â€” skipping quality analysis."
            echo "$EMPTY" > /tmp/response_quality.json
            exit 0
          fi

          jq '. + {"analysis_type": "quality"}' /tmp/review_payload.json > /tmp/payload_quality.json
          MAX_RETRIES=3; RETRY_DELAY=10; ATTEMPT=0; SUCCESS=false

          while [ $ATTEMPT -lt $MAX_RETRIES ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo "::group::Quality analysis â€” attempt ${ATTEMPT}/${MAX_RETRIES}"
            HTTP_STATUS=$(curl -s --max-time 300 --retry 0 \
              -o /tmp/response_quality.json -w "%{http_code}" \
              -X POST "${API_URL}/api/review/pr" \
              -H "Content-Type: application/json" \
              -H "x-api-key: ${API_KEY}" \
              -d @/tmp/payload_quality.json)
            echo "HTTP status: ${HTTP_STATUS}"
            echo "::endgroup::"
            if [ "$HTTP_STATUS" = "200" ]; then
              SUCCESS=true; break
            elif [ "$HTTP_STATUS" = "429" ] || [ "$HTTP_STATUS" -ge "500" ] 2>/dev/null; then
              echo "Transient error (HTTP ${HTTP_STATUS}). Waiting ${RETRY_DELAY}s before retry..."
              sleep "$RETRY_DELAY"; RETRY_DELAY=$((RETRY_DELAY * 2))
            else
              echo "::error::Quality analysis API error: HTTP ${HTTP_STATUS}"
              cat /tmp/response_quality.json; exit 1
            fi
          done

          if [ "$SUCCESS" != "true" ]; then
            echo "::error::Quality analysis failed after ${MAX_RETRIES} attempts."
            exit 1
          fi

          echo "Quality analysis complete."
          jq '{total_issues: .total_issues, severity: .severity_breakdown}' /tmp/response_quality.json 2>/dev/null || true

      # â”€â”€ Step 9: Aggregate all code review results â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Aggregate code review results
        id: aggregate
        if: always() && steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        run: |
          python3 << 'PYEOF'
          import json, os, sys

          responses = {
            'complexity': '/tmp/response_complexity.json',
            'security':   '/tmp/response_security.json',
            'review':     '/tmp/response_review.json',
            'quality':    '/tmp/response_quality.json',
          }

          all_validation_failures = []
          all_llm_findings        = []
          all_per_file_results    = []
          severity_totals = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0, 'info': 0}
          total_issues        = 0
          total_input_tokens  = 0
          total_output_tokens = 0
          total_cost_usd      = 0.0
          total_latency_ms    = 0
          gate_fail           = False
          run_id              = ''
          model               = ''
          files_reviewed      = 0
          timestamp           = ''
          per_analysis        = {}

          for analysis_type, path in responses.items():
            if not os.path.isfile(path):
              print(f"Warning: missing response file {path}", file=sys.stderr)
              per_analysis[analysis_type] = {}
              continue
            with open(path) as f:
              r = json.load(f)
            per_analysis[analysis_type] = r

            vf = r.get('validation_failures', [])
            for item in vf:
              item['analysis_type'] = analysis_type
            all_validation_failures.extend(vf)

            lf = r.get('llm_findings', [])
            for item in lf:
              item['analysis_type'] = analysis_type
            all_llm_findings.extend(lf)

            all_per_file_results.extend(r.get('per_file_results', []))

            sb = r.get('severity_breakdown', {})
            for sev in severity_totals:
              severity_totals[sev] += int(sb.get(sev, 0))

            total_issues        += int(r.get('total_issues', 0))
            tu = r.get('token_usage', {})
            total_input_tokens  += int(tu.get('input_tokens', 0))
            total_output_tokens += int(tu.get('output_tokens', 0))
            total_cost_usd      += float(r.get('cost_usd', 0))
            total_latency_ms    += int(r.get('latency_ms', 0))

            if r.get('gate_status') == 'fail':
              gate_fail = True
            if not run_id and r.get('run_id'):
              run_id = r['run_id']
            if not model and r.get('model'):
              model = r['model']
            if not files_reviewed and r.get('files_reviewed'):
              files_reviewed = r['files_reviewed']
            if not timestamp and r.get('timestamp'):
              timestamp = r['timestamp']

          aggregated = {
            'run_id':              run_id,
            'gate_status':         'fail' if gate_fail else 'pass',
            'status':              'failure' if gate_fail else 'success',
            'total_issues':        total_issues,
            'severity_breakdown':  severity_totals,
            'validation_failures': all_validation_failures,
            'llm_findings':        all_llm_findings,
            'per_file_results':    all_per_file_results,
            'token_usage': {
              'input_tokens':  total_input_tokens,
              'output_tokens': total_output_tokens,
              'total_tokens':  total_input_tokens + total_output_tokens,
            },
            'cost_usd':       total_cost_usd,
            'latency_ms':     total_latency_ms,
            'model':          model,
            'files_reviewed': files_reviewed,
            'timestamp':      timestamp,
            'per_analysis':   per_analysis,
          }

          with open('/tmp/review_response.json', 'w') as f:
            json.dump(aggregated, f)

          with open(os.environ['GITHUB_OUTPUT'], 'a') as gf:
            gf.write(f"gate_status={'fail' if gate_fail else 'pass'}\n")
            gf.write(f"status={'failure' if gate_fail else 'success'}\n")
            gf.write(f"total_issues={total_issues}\n")
            gf.write(f"critical={severity_totals['critical']}\n")
            gf.write(f"high={severity_totals['high']}\n")
            gf.write(f"validation_failures={len(all_validation_failures)}\n")

          print(
            f"Aggregation complete: {total_issues} total issues, "
            f"{severity_totals['critical']} critical, "
            f"{len(all_validation_failures)} validation failures, "
            f"gate={'FAIL' if gate_fail else 'PASS'}",
            file=sys.stderr
          )
          PYEOF

      # â”€â”€ Step 10: Post structured PR comment â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Post PR review comment
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true' && always()
        uses: actions/github-script@v7
        env:
          AI_AGENT_API_URL: ${{ secrets.AI_AGENT_API_URL }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');

            const responseFile = '/tmp/review_response.json';
            if (!fs.existsSync(responseFile)) {
              console.log('No review response file â€” skipping PR comment.');
              return;
            }

            const r = JSON.parse(fs.readFileSync(responseFile, 'utf8'));
            const gatePass  = r.gate_status === 'pass';
            const gateEmoji = gatePass ? 'âœ…' : 'âŒ';
            const gateLabel = gatePass ? 'PASSED' : 'FAILED';

            const pr      = context.payload.pull_request;
            const headSha = pr.head.sha.slice(0, 8);
            const sb      = r.severity_breakdown || {};
            const tu      = r.token_usage || {};
            const pa      = r.per_analysis || {};

            let body = `## ${gateEmoji} AI Code Review Gate â€” ${gateLabel}\n\n`;
            body += `| Field | Value |\n|---|---|\n`;
            body += `| Repository | \`${context.repo.owner}/${context.repo.repo}\` |\n`;
            body += `| Branch | \`${pr.head.ref}\` |\n`;
            body += `| Commit | \`${headSha}\` |\n`;
            body += `| Files Reviewed | ${r.files_reviewed} |\n`;
            body += `| Total Run Time | ${((r.latency_ms || 0) / 1000).toFixed(1)}s |\n`;
            body += `| Run ID | \`${r.run_id}\` |\n\n`;

            // Overall severity table
            body += `### Severity Breakdown (All Stages)\n\n`;
            body += `| ğŸ”´ Critical | ğŸŸ  High | ğŸŸ¡ Medium | ğŸ”µ Low | âšª Info |\n`;
            body += `|:---:|:---:|:---:|:---:|:---:|\n`;
            body += `| ${sb.critical||0} | ${sb.high||0} | ${sb.medium||0} | ${sb.low||0} | ${sb.info||0} |\n\n`;

            // Stage results summary table
            const stages = [
              { key: 'complexity', label: 'ğŸ” Code Complexity Analysis' },
              { key: 'security',   label: 'ğŸ”’ AI Security Scan'         },
              { key: 'review',     label: 'ğŸ¤– AI Code Review'           },
              { key: 'quality',    label: 'ğŸ“ AI Code Quality Analysis' },
            ];

            body += `### Stage Results Summary\n\n`;
            body += `| Stage | Issues | Critical | High | Validation Failures |\n`;
            body += `|---|:---:|:---:|:---:|:---:|\n`;
            for (const s of stages) {
              const sr  = pa[s.key] || {};
              const ssb = sr.severity_breakdown || {};
              const vfc = (sr.validation_failures || []).length;
              body += `| ${s.label} | ${sr.total_issues||0} | ${ssb.critical||0} | ${ssb.high||0} | ${vfc} |\n`;
            }
            body += `\n`;

            // Per-stage detailed findings (collapsible)
            for (const s of stages) {
              const sr  = pa[s.key] || {};
              const vf  = sr.validation_failures || [];
              const lf  = sr.llm_findings || [];
              const critHighLLM = lf.filter(f => f.severity === 'critical' || f.severity === 'high');
              const medLowLLM   = lf.filter(f => ['medium', 'low', 'info'].includes(f.severity));
              if (vf.length === 0 && lf.length === 0) continue;

              body += `<details>\n<summary>${s.label} â€” ${sr.total_issues||0} issue(s)</summary>\n\n`;

              if (vf.length > 0) {
                body += `#### âŒ Validation Failures (${vf.length})\n\n`;
                for (const f of vf.slice(0, 15)) {
                  const sev = (f.severity || 'unknown').toUpperCase();
                  const loc = f.line_number ? ` (line ${f.line_number})` : '';
                  body += `- **[${sev}]** \`${f.file}${loc}\`\n`;
                  body += `  **${f.message}**\n`;
                  body += `  > ğŸ’¡ ${f.suggestion}\n\n`;
                }
                if (vf.length > 15) body += `_...and ${vf.length - 15} more._\n\n`;
              }

              if (critHighLLM.length > 0) {
                body += `#### ğŸš¨ Critical & High Findings (${critHighLLM.length})\n\n`;
                for (const f of critHighLLM.slice(0, 10)) {
                  const sev = (f.severity || 'unknown').toUpperCase();
                  const loc = f.line_number ? ` (line ${f.line_number})` : '';
                  body += `- **[${sev}]** \`${f.file}${loc}\` â€” ${f.category}\n`;
                  body += `  **${f.message}**\n`;
                  if (f.suggestion) body += `  > ğŸ’¡ ${f.suggestion}\n`;
                  if (f.reasoning)  body += `  > ğŸ” ${f.reasoning}\n`;
                  body += `\n`;
                }
              }

              if (medLowLLM.length > 0) {
                body += `#### âš ï¸ Medium & Lower Findings (${medLowLLM.length})\n\n`;
                for (const f of medLowLLM.slice(0, 10)) {
                  const sev = (f.severity || 'unknown').toUpperCase();
                  const loc = f.line_number ? ` (line ${f.line_number})` : '';
                  body += `- **[${sev}]** \`${f.file}${loc}\` â€” ${f.message}\n`;
                  if (f.suggestion) body += `  > ğŸ’¡ ${f.suggestion}\n`;
                  body += `\n`;
                }
                if (medLowLLM.length > 10) body += `_...and ${medLowLLM.length - 10} more._\n\n`;
              }

              body += `</details>\n\n`;
            }

            // All validation failures consolidated (blocking)
            const allVf = r.validation_failures || [];
            if (allVf.length > 0) {
              body += `### âŒ Validation Rule Violations (${allVf.length}) â€” **Blocking**\n\n`;
              body += `> These violations are deterministic rule failures that block merge.\n\n`;
              for (const f of allVf.slice(0, 25)) {
                const sev   = (f.severity || 'unknown').toUpperCase();
                const loc   = f.line_number ? ` (line ${f.line_number})` : '';
                const stage = f.analysis_type ? ` [${f.analysis_type}]` : '';
                body += `- **[${sev}]**${stage} \`${f.file}${loc}\`\n`;
                body += `  **${f.message}**\n`;
                body += `  > ğŸ’¡ ${f.suggestion}\n\n`;
              }
              if (allVf.length > 25) {
                body += `_...and ${allVf.length - 25} more violations. Fix all before merging._\n\n`;
              }
            } else {
              body += `### âœ… Validation Rules â€” All Passed\n\n`;
            }

            // LLM findings â€” critical/high (blocking if critical)
            const llm         = r.llm_findings || [];
            const critHighLLM = llm.filter(f => f.severity === 'critical' || f.severity === 'high');
            if (critHighLLM.length > 0) {
              body += `### ğŸ¤– AI Findings â€” Critical & High (${critHighLLM.length})\n\n`;
              for (const f of critHighLLM.slice(0, 15)) {
                const sev   = (f.severity || 'unknown').toUpperCase();
                const loc   = f.line_number ? ` (line ${f.line_number})` : '';
                const stage = f.analysis_type ? ` [${f.analysis_type}]` : '';
                body += `- **[${sev}]**${stage} \`${f.file}${loc}\` â€” ${f.category}\n`;
                body += `  **${f.message}**\n`;
                if (f.suggestion) body += `  > ğŸ’¡ ${f.suggestion}\n`;
                if (f.reasoning)  body += `  > ğŸ” ${f.reasoning}\n`;
                body += `\n`;
              }
            }

            // LLM findings â€” medium/low (collapsible)
            const medLowLLM = llm.filter(f => ['medium', 'low', 'info'].includes(f.severity));
            if (medLowLLM.length > 0) {
              body += `<details>\n<summary>ğŸ¤– AI Findings â€” Medium & Lower (${medLowLLM.length})</summary>\n\n`;
              for (const f of medLowLLM.slice(0, 30)) {
                const sev   = (f.severity || 'unknown').toUpperCase();
                const loc   = f.line_number ? ` (line ${f.line_number})` : '';
                const stage = f.analysis_type ? ` [${f.analysis_type}]` : '';
                body += `- **[${sev}]**${stage} \`${f.file}${loc}\` â€” ${f.message}\n`;
                if (f.suggestion) body += `  > ğŸ’¡ ${f.suggestion}\n`;
                body += `\n`;
              }
              body += `</details>\n\n`;
            }

            if (llm.length === 0 && allVf.length === 0) {
              body += `### ğŸ¤– AI Analysis â€” No Issues Found\n\n`;
              body += `The AI reviewer found no issues in the changed files. Great work!\n\n`;
            }

            // Per-file summary (collapsible)
            const pfr = r.per_file_results || [];
            if (pfr.length > 0) {
              body += `<details>\n<summary>ğŸ“ Per-File Results (${pfr.length} files)</summary>\n\n`;
              body += `| File | Language | Validation | LLM | Errors |\n`;
              body += `|---|---|:---:|:---:|:---:|\n`;
              for (const f of pfr) {
                const errIcon = f.error ? 'âš ï¸' : '';
                body += `| \`${f.file}\` | ${f.language} | ${f.validation_count} | ${f.llm_count} | ${errIcon} |\n`;
              }
              body += `\n</details>\n\n`;
            }

            // Token usage and cost
            body += `### ğŸ“Š Token Usage & Cost (All Stages)\n\n`;
            body += `| Metric | Value |\n|---|---|\n`;
            body += `| Input Tokens | ${(tu.input_tokens||0).toLocaleString()} |\n`;
            body += `| Output Tokens | ${(tu.output_tokens||0).toLocaleString()} |\n`;
            body += `| Total Tokens | ${(tu.total_tokens||0).toLocaleString()} |\n`;
            body += `| Model | \`${r.model||'unknown'}\` |\n`;
            body += `| Total Cost | \`$${(r.cost_usd||0).toFixed(4)}\` |\n\n`;

            body += `---\n`;
            body += `*ğŸ¤– Powered by [AI Code Review Assistant](${process.env.AI_AGENT_API_URL}) `;
            body += `| Run \`${r.run_id}\` | ${new Date().toUTCString()}*\n`;

            await github.rest.issues.createComment({
              owner:        context.repo.owner,
              repo:         context.repo.repo,
              issue_number: pr.number,
              body,
            });

      # â”€â”€ Step 11: Emit summary metrics to observability backend â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Report summary metrics
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true' && always()
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
          REPO: ${{ github.repository }}
          PROJECT_ID: ${{ github.event.repository.name }}
          WORKFLOW_RUN_ID: ${{ github.run_id }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ github.head_ref }}
          ACTOR: ${{ github.actor }}
        run: |
          if [ ! -f /tmp/review_response.json ]; then
            echo "No review response â€” skipping metrics."
            exit 0
          fi

          R=$(cat /tmp/review_response.json)

          METRIC_PAYLOAD=$(jq -n \
            --arg  repo          "$REPO" \
            --arg  project_id    "$PROJECT_ID" \
            --arg  run_id        "$WORKFLOW_RUN_ID" \
            --argjson pr_number  "${PR_NUMBER:-0}" \
            --arg  branch        "$BRANCH" \
            --arg  actor         "$ACTOR" \
            --argjson response   "$R" \
            '{
              repository:      $repo,
              project_id:      $project_id,
              workflow_run_id: $run_id,
              pr_number:       $pr_number,
              branch:          $branch,
              triggered_by:    $actor,
              source:          "pipeline",
              status:          $response.status,
              files_reviewed:  ($response.files_reviewed // 0),
              issues_found:    ($response.total_issues // 0),
              critical_count:  ($response.severity_breakdown.critical // 0),
              high_count:      ($response.severity_breakdown.high // 0),
              input_tokens:    ($response.token_usage.input_tokens // 0),
              output_tokens:   ($response.token_usage.output_tokens // 0),
              total_tokens:    ($response.token_usage.total_tokens // 0),
              latency_ms:      ($response.latency_ms // 0),
              model:           ($response.model // ""),
              cost_usd:        ($response.cost_usd // 0)
            }')

          curl -s -o /dev/null -w "Metrics ingest: HTTP %{http_code}\n" \
            --max-time 30 \
            -X POST "${API_URL}/api/metrics/ingest" \
            -H "Content-Type: application/json" \
            -H "x-api-key: ${API_KEY}" \
            -d "$METRIC_PAYLOAD" || echo "Warning: summary metrics ingest failed (non-blocking)"

      # â”€â”€ Step 12: Emit full drill-down detail to observability backend â”€â”€â”€â”€â”€â”€
      - name: Report drill-down detail
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true' && always()
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
          REPO: ${{ github.repository }}
          PROJECT_ID: ${{ github.event.repository.name }}
          WORKFLOW_RUN_ID: ${{ github.run_id }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          COMMIT_SHA: ${{ github.sha }}
          BRANCH: ${{ github.head_ref }}
          BASE_BRANCH: ${{ github.base_ref }}
          ACTOR: ${{ github.actor }}
        run: |
          if [ ! -f /tmp/review_response.json ]; then
            echo "No review response â€” skipping drill-down detail."
            exit 0
          fi

          R=$(cat /tmp/review_response.json)

          DETAIL_PAYLOAD=$(jq -n \
            --arg  repo          "$REPO" \
            --arg  project_id    "$PROJECT_ID" \
            --arg  run_id        "$WORKFLOW_RUN_ID" \
            --argjson pr_number  "${PR_NUMBER:-0}" \
            --arg  commit_sha    "$COMMIT_SHA" \
            --arg  branch        "$BRANCH" \
            --arg  base_branch   "$BASE_BRANCH" \
            --arg  actor         "$ACTOR" \
            --argjson response   "$R" \
            '{
              run_id:               $response.run_id,
              repository:           $repo,
              project_id:           $project_id,
              workflow_run_id:      $run_id,
              pr_number:            $pr_number,
              commit_sha:           $commit_sha,
              branch:               $branch,
              base_branch:          $base_branch,
              actor:                $actor,
              source:               "pipeline",
              gate_status:          ($response.gate_status // "pass"),
              status:               ($response.status // "success"),
              validation_failures:  ($response.validation_failures // []),
              llm_findings:         ($response.llm_findings // []),
              per_file_results:     ($response.per_file_results // []),
              severity_distribution: ($response.severity_breakdown // {}),
              total_issues:         ($response.total_issues // 0),
              critical_count:       ($response.severity_breakdown.critical // 0),
              high_count:           ($response.severity_breakdown.high // 0),
              medium_count:         ($response.severity_breakdown.medium // 0),
              low_count:            ($response.severity_breakdown.low // 0),
              files_reviewed:       ($response.files_reviewed // 0),
              input_tokens:         ($response.token_usage.input_tokens // 0),
              output_tokens:        ($response.token_usage.output_tokens // 0),
              total_tokens:         ($response.token_usage.total_tokens // 0),
              cost_usd:             ($response.cost_usd // 0),
              model:                ($response.model // ""),
              latency_ms:           ($response.latency_ms // 0),
              timestamp:            ($response.timestamp // ""),
              runtime_ms:           ($response.latency_ms // 0)
            }')

          curl -s -o /dev/null -w "Drill-down detail: HTTP %{http_code}\n" \
            --max-time 30 \
            -X POST "${API_URL}/api/metrics/runs" \
            -H "Content-Type: application/json" \
            -H "x-api-key: ${API_KEY}" \
            -d "$DETAIL_PAYLOAD" || echo "Warning: drill-down detail storage failed (non-blocking)"

      # â”€â”€ Step 13: Quality gate â€” enforce standards â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      - name: Quality gate â€” enforce standards
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        env:
          GATE_STATUS:      ${{ steps.aggregate.outputs.gate_status }}
          CRITICAL:         ${{ steps.aggregate.outputs.critical }}
          VAL_FAILURES:     ${{ steps.aggregate.outputs.validation_failures }}
        run: |
          if [ "$GATE_STATUS" = "fail" ]; then
            echo "::error title=AI Code Review Gate FAILED::Found ${CRITICAL} critical issue(s) and ${VAL_FAILURES} validation failure(s)."
            echo "::error::Review the PR comment for a full breakdown. Fix all blocking issues before merging."
            exit 1
          fi
          echo "âœ… AI Code Review Gate PASSED â€” no blocking issues found."
