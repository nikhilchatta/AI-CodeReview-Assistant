name: AI Code Review Gate

on:
  workflow_call:
    inputs:
      max_files:
        description: Maximum number of changed files to send for review
        type: number
        default: 20
      file_extensions:
        description: Comma-separated file extensions to include (e.g. .py,.ts,.scala)
        type: string
        default: .py,.ts,.tsx,.js,.jsx,.scala,.sql,.tf
      timeout_minutes:
        description: Maximum minutes the review job is allowed to run
        type: number
        default: 15
    secrets:
      AI_AGENT_API_URL:
        required: true
        description: Base URL of the AWS-hosted AI agent backend (e.g. https://review.example.com)
      AI_AGENT_API_KEY:
        required: true
        description: API key for authenticating with the AI agent backend

jobs:
  ai-code-review-gate:
    name: AI Code Review Gate
    runs-on: ubuntu-latest
    timeout-minutes: ${{ inputs.timeout_minutes }}
    permissions:
      contents: read
      pull-requests: write

    steps:
      # ‚îÄ‚îÄ Step 1: Checkout PR code with full history for diff ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Checkout PR code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # ‚îÄ‚îÄ Step 2: Compute changed files from PR diff ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Compute changed files from PR diff
        id: diff
        env:
          BASE_REF: ${{ github.base_ref }}
          MAX_FILES: ${{ inputs.max_files }}
          EXTENSIONS: ${{ inputs.file_extensions }}
        run: |
          git fetch origin "${BASE_REF}" --depth=1

          # Build glob patterns from extension list (e.g. ".py,.ts" -> "*.py *.ts")
          PATTERNS=$(echo "$EXTENSIONS" | tr ',' '\n' | sed 's/^\./\*./g' | tr '\n' ' ')

          # Get changed/added/modified files only (not deleted), capped at max_files
          FILES=$(git diff --name-only --diff-filter=ACMRT \
            "origin/${BASE_REF}...HEAD" \
            -- $PATTERNS 2>/dev/null | head -n "$MAX_FILES")

          if [ -z "$FILES" ]; then
            echo "no_files=true"  >> "$GITHUB_OUTPUT"
            echo "No relevant files changed in this PR. Skipping AI review."
          else
            echo "no_files=false" >> "$GITHUB_OUTPUT"
            echo "$FILES" > /tmp/changed_files.txt
            FILE_COUNT=$(echo "$FILES" | wc -l | tr -d ' ')
            echo "file_count=${FILE_COUNT}" >> "$GITHUB_OUTPUT"
            echo "Found ${FILE_COUNT} file(s) to review:"
            cat /tmp/changed_files.txt
          fi

      # ‚îÄ‚îÄ Step 3: Early exit when no reviewable files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Skip ‚Äî no reviewable files changed
        if: steps.diff.outputs.no_files == 'true'
        run: |
          echo "No files matching [ ${{ inputs.file_extensions }} ] were changed."
          echo "AI code review gate skipped ‚Äî nothing to review."

      # ‚îÄ‚îÄ Step 4: Build JSON payload from changed files ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Build review payload
        id: payload
        if: steps.diff.outputs.no_files == 'false'
        env:
          PR_NUMBER: ${{ github.event.pull_request.number }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_HEAD_REF: ${{ github.head_ref }}
          GITHUB_BASE_REF: ${{ github.base_ref }}
          GITHUB_SHA: ${{ github.sha }}
          GITHUB_ACTOR: ${{ github.actor }}
          GITHUB_RUN_ID: ${{ github.run_id }}
        run: |
          python3 << 'PYEOF'
          import json, os, sys

          EXT_LANG = {
            '.py':    'python',
            '.pyspark': 'pyspark',
            '.ts':    'typescript',
            '.tsx':   'typescript',
            '.js':    'javascript',
            '.jsx':   'javascript',
            '.scala': 'scala',
            '.sql':   'sql',
            '.tf':    'terraform',
          }

          # Use GITHUB_WORKSPACE for reliable absolute path resolution
          workspace = os.environ.get('GITHUB_WORKSPACE', '')

          files = []
          with open('/tmp/changed_files.txt') as fh:
            paths = [line.strip() for line in fh if line.strip()]

          for fpath in paths:
            full_path = os.path.join(workspace, fpath) if workspace else fpath
            if not os.path.isfile(full_path):
              print(f"Skip (deleted/missing): {fpath} (resolved: {full_path})", file=sys.stderr)
              continue
            ext = os.path.splitext(fpath)[1].lower()
            language = EXT_LANG.get(ext)
            if not language:
              continue
            try:
              with open(full_path, 'r', encoding='utf-8', errors='replace') as f:
                content = f.read()
              # Truncate large files to 100 KB to keep tokens manageable
              if len(content) > 102400:
                content = content[:102400]
                print(f"Truncated {fpath} to 100 KB", file=sys.stderr)
              files.append({'path': fpath, 'content': content, 'language': language})
            except Exception as e:
              print(f"Warning ‚Äî could not read {fpath}: {e}", file=sys.stderr)

          if not files:
            print("No readable files found after filtering.", file=sys.stderr)
            with open(os.environ['GITHUB_OUTPUT'], 'a') as gf:
              gf.write("no_files=true\n")
            sys.exit(0)

          payload = {
            'files': files,
            'metadata': {
              'repository':      os.environ.get('GITHUB_REPOSITORY', ''),
              'pr_number':       int(os.environ.get('PR_NUMBER') or 0),
              'branch':          os.environ.get('GITHUB_HEAD_REF', ''),
              'base_branch':     os.environ.get('GITHUB_BASE_REF', ''),
              'commit_sha':      os.environ.get('GITHUB_SHA', ''),
              'actor':           os.environ.get('GITHUB_ACTOR', ''),
              'workflow_run_id': os.environ.get('GITHUB_RUN_ID', ''),
              'project_id':      os.environ.get('GITHUB_REPOSITORY', '').split('/')[-1],
            }
          }

          with open('/tmp/review_payload.json', 'w') as f:
            json.dump(payload, f)

          print(f"Payload ready: {len(files)} file(s)", file=sys.stderr)
          PYEOF

      # ‚îÄ‚îÄ Step 5: Call AWS AI agent with exponential-backoff retry ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Call AI Review Agent
        id: review
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
        run: |
          # If payload file is missing or empty, no reviewable files were found
          if [ ! -f /tmp/review_payload.json ]; then
            echo "No payload file ‚Äî no files matched supported languages. Skipping."
            echo "gate_status=pass"        >> "$GITHUB_OUTPUT"
            echo "status=success"          >> "$GITHUB_OUTPUT"
            echo "total_issues=0"          >> "$GITHUB_OUTPUT"
            echo "critical=0"              >> "$GITHUB_OUTPUT"
            echo "high=0"                  >> "$GITHUB_OUTPUT"
            echo "validation_failures=0"   >> "$GITHUB_OUTPUT"
            exit 0
          fi

          # Debug: show payload summary
          FILE_COUNT=$(jq '.files | length' /tmp/review_payload.json 2>/dev/null || echo "invalid JSON")
          PAYLOAD_SIZE=$(wc -c < /tmp/review_payload.json)
          echo "Payload: ${FILE_COUNT} file(s), ${PAYLOAD_SIZE} bytes"
          jq '{file_count: (.files|length), files: [.files[].path], metadata: .metadata}' /tmp/review_payload.json 2>/dev/null || echo "Failed to parse payload JSON"

          MAX_RETRIES=3
          RETRY_DELAY=10
          ATTEMPT=0
          SUCCESS=false

          while [ $ATTEMPT -lt $MAX_RETRIES ]; do
            ATTEMPT=$((ATTEMPT + 1))
            echo "::group::API attempt ${ATTEMPT}/${MAX_RETRIES}"

            HTTP_STATUS=$(curl -s \
              --max-time 300 \
              --retry 0 \
              -o /tmp/review_response.json \
              -w "%{http_code}" \
              -X POST "${API_URL}/api/review/pr" \
              -H "Content-Type: application/json" \
              -H "x-api-key: ${API_KEY}" \
              -d @/tmp/review_payload.json)

            echo "HTTP status: ${HTTP_STATUS}"
            echo "::endgroup::"

            if [ "$HTTP_STATUS" = "200" ]; then
              SUCCESS=true
              break
            elif [ "$HTTP_STATUS" = "429" ] || [ "$HTTP_STATUS" -ge "500" ] 2>/dev/null; then
              echo "Transient error (HTTP ${HTTP_STATUS}). Waiting ${RETRY_DELAY}s before retry..."
              sleep "$RETRY_DELAY"
              RETRY_DELAY=$((RETRY_DELAY * 2))
            else
              echo "::error::Non-retriable API error: HTTP ${HTTP_STATUS}"
              cat /tmp/review_response.json
              exit 1
            fi
          done

          if [ "$SUCCESS" != "true" ]; then
            echo "::error::AI Review API failed after ${MAX_RETRIES} attempts."
            exit 1
          fi

          # Validate response structure
          if ! jq -e '.gate_status' /tmp/review_response.json > /dev/null 2>&1; then
            echo "::error::API response is missing gate_status field. Response:"
            cat /tmp/review_response.json
            exit 1
          fi

          # Export key values for downstream steps
          GATE_STATUS=$(jq -r '.gate_status'                      /tmp/review_response.json)
          STATUS=$(jq -r '.status'                                 /tmp/review_response.json)
          TOTAL_ISSUES=$(jq -r '.total_issues'                     /tmp/review_response.json)
          CRITICAL=$(jq -r '.severity_breakdown.critical // 0'     /tmp/review_response.json)
          HIGH=$(jq -r '.severity_breakdown.high // 0'             /tmp/review_response.json)
          VAL_FAILURES=$(jq -r '.validation_failures | length'     /tmp/review_response.json)

          {
            echo "gate_status=${GATE_STATUS}"
            echo "status=${STATUS}"
            echo "total_issues=${TOTAL_ISSUES}"
            echo "critical=${CRITICAL}"
            echo "high=${HIGH}"
            echo "validation_failures=${VAL_FAILURES}"
          } >> "$GITHUB_OUTPUT"

          echo "Review complete: gate=${GATE_STATUS}, issues=${TOTAL_ISSUES} (${CRITICAL}C / ${HIGH}H), validation_failures=${VAL_FAILURES}"

      # ‚îÄ‚îÄ Step 6: Post structured PR comment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Post PR review comment
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true' && always()
        uses: actions/github-script@v7
        env:
          AI_AGENT_API_URL: ${{ secrets.AI_AGENT_API_URL }}
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs   = require('fs');
            const path = require('path');

            const responseFile = '/tmp/review_response.json';
            if (!fs.existsSync(responseFile)) {
              console.log('No review response file ‚Äî skipping PR comment.');
              return;
            }

            const r = JSON.parse(fs.readFileSync(responseFile, 'utf8'));
            const gatePass  = r.gate_status === 'pass';
            const gateEmoji = gatePass ? '‚úÖ' : '‚ùå';
            const gateLabel = gatePass ? 'PASSED' : 'FAILED';

            const pr      = context.payload.pull_request;
            const headSha = pr.head.sha.slice(0, 8);
            const sb      = r.severity_breakdown || {};
            const tu      = r.token_usage || {};

            let body = `## ${gateEmoji} AI Code Review Gate ‚Äî ${gateLabel}\n\n`;
            body += `| Field | Value |\n|---|---|\n`;
            body += `| Repository | \`${context.repo.owner}/${context.repo.repo}\` |\n`;
            body += `| Branch | \`${pr.head.ref}\` |\n`;
            body += `| Commit | \`${headSha}\` |\n`;
            body += `| Files Reviewed | ${r.files_reviewed} |\n`;
            body += `| Run Time | ${((r.latency_ms || 0) / 1000).toFixed(1)}s |\n`;
            body += `| Run ID | \`${r.run_id}\` |\n\n`;

            // Severity table
            body += `### Severity Breakdown\n\n`;
            body += `| üî¥ Critical | üü† High | üü° Medium | üîµ Low | ‚ö™ Info |\n`;
            body += `|:---:|:---:|:---:|:---:|:---:|\n`;
            body += `| ${sb.critical||0} | ${sb.high||0} | ${sb.medium||0} | ${sb.low||0} | ${sb.info||0} |\n\n`;

            // Validation failures (blocking)
            const vf = r.validation_failures || [];
            if (vf.length > 0) {
              body += `### ‚ùå Validation Rule Violations (${vf.length}) ‚Äî **Blocking**\n\n`;
              body += `> These violations are deterministic rule failures that block merge.\n\n`;
              for (const f of vf.slice(0, 25)) {
                const sev  = (f.severity || 'unknown').toUpperCase();
                const loc  = f.line_number ? ` (line ${f.line_number})` : '';
                body += `- **[${sev}]** \`${f.file}${loc}\`\n`;
                body += `  **${f.message}**\n`;
                body += `  > üí° ${f.suggestion}\n\n`;
              }
              if (vf.length > 25) {
                body += `_...and ${vf.length - 25} more violations. Fix all before merging._\n\n`;
              }
            } else {
              body += `### ‚úÖ Validation Rules ‚Äî All Passed\n\n`;
            }

            // LLM findings ‚Äî critical/high (blocking if critical)
            const llm        = r.llm_findings || [];
            const critHighLLM = llm.filter(f => f.severity === 'critical' || f.severity === 'high');
            if (critHighLLM.length > 0) {
              body += `### ü§ñ AI Findings ‚Äî Critical & High (${critHighLLM.length})\n\n`;
              for (const f of critHighLLM.slice(0, 15)) {
                const sev = (f.severity || 'unknown').toUpperCase();
                const loc = f.line_number ? ` (line ${f.line_number})` : '';
                body += `- **[${sev}]** \`${f.file}${loc}\` ‚Äî ${f.category}\n`;
                body += `  **${f.message}**\n`;
                if (f.suggestion) body += `  > üí° ${f.suggestion}\n`;
                if (f.reasoning)  body += `  > üîç ${f.reasoning}\n`;
                body += `\n`;
              }
            }

            // LLM findings ‚Äî medium/low (collapsible)
            const medLowLLM = llm.filter(f => ['medium', 'low', 'info'].includes(f.severity));
            if (medLowLLM.length > 0) {
              body += `<details>\n<summary>ü§ñ AI Findings ‚Äî Medium & Lower (${medLowLLM.length})</summary>\n\n`;
              for (const f of medLowLLM.slice(0, 30)) {
                const sev = (f.severity || 'unknown').toUpperCase();
                const loc = f.line_number ? ` (line ${f.line_number})` : '';
                body += `- **[${sev}]** \`${f.file}${loc}\` ‚Äî ${f.message}\n`;
                if (f.suggestion) body += `  > üí° ${f.suggestion}\n`;
                body += `\n`;
              }
              body += `</details>\n\n`;
            }

            if (llm.length === 0 && vf.length === 0) {
              body += `### ü§ñ AI Analysis ‚Äî No Issues Found\n\n`;
              body += `The AI reviewer found no issues in the changed files. Great work!\n\n`;
            }

            // Per-file summary (collapsible)
            const pfr = r.per_file_results || [];
            if (pfr.length > 0) {
              body += `<details>\n<summary>üìÅ Per-File Results (${pfr.length} files)</summary>\n\n`;
              body += `| File | Language | Validation | LLM | Errors |\n`;
              body += `|---|---|:---:|:---:|:---:|\n`;
              for (const f of pfr) {
                const errIcon = f.error ? '‚ö†Ô∏è' : '';
                body += `| \`${f.file}\` | ${f.language} | ${f.validation_count} | ${f.llm_count} | ${errIcon} |\n`;
              }
              body += `\n</details>\n\n`;
            }

            // Token usage and cost
            body += `### üìä Token Usage & Cost\n\n`;
            body += `| Metric | Value |\n|---|---|\n`;
            body += `| Input Tokens | ${(tu.input_tokens||0).toLocaleString()} |\n`;
            body += `| Output Tokens | ${(tu.output_tokens||0).toLocaleString()} |\n`;
            body += `| Total Tokens | ${(tu.total_tokens||0).toLocaleString()} |\n`;
            body += `| Model | \`${r.model||'unknown'}\` |\n`;
            body += `| Cost | \`$${(r.cost_usd||0).toFixed(4)}\` |\n\n`;

            // Footer
            body += `---\n`;
            body += `*ü§ñ Powered by [AI Code Review Assistant](${process.env.AI_AGENT_API_URL}) `;
            body += `| Run \`${r.run_id}\` | ${new Date().toUTCString()}*\n`;

            await github.rest.issues.createComment({
              owner:        context.repo.owner,
              repo:         context.repo.repo,
              issue_number: pr.number,
              body,
            });

      # ‚îÄ‚îÄ Step 7: Emit summary metrics to observability backend ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Report summary metrics
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true' && always()
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
          REPO: ${{ github.repository }}
          PROJECT_ID: ${{ github.event.repository.name }}
          WORKFLOW_RUN_ID: ${{ github.run_id }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          BRANCH: ${{ github.head_ref }}
          ACTOR: ${{ github.actor }}
        run: |
          if [ ! -f /tmp/review_response.json ]; then
            echo "No review response ‚Äî skipping metrics."
            exit 0
          fi

          R=$(cat /tmp/review_response.json)

          # Build summary metric payload using jq
          METRIC_PAYLOAD=$(jq -n \
            --arg  repo          "$REPO" \
            --arg  project_id    "$PROJECT_ID" \
            --arg  run_id        "$WORKFLOW_RUN_ID" \
            --argjson pr_number  "${PR_NUMBER:-0}" \
            --arg  branch        "$BRANCH" \
            --arg  actor         "$ACTOR" \
            --argjson response   "$R" \
            '{
              repository:      $repo,
              project_id:      $project_id,
              workflow_run_id: $run_id,
              pr_number:       $pr_number,
              branch:          $branch,
              triggered_by:    $actor,
              source:          "pipeline",
              status:          $response.status,
              files_reviewed:  ($response.files_reviewed // 0),
              issues_found:    ($response.total_issues // 0),
              critical_count:  ($response.severity_breakdown.critical // 0),
              high_count:      ($response.severity_breakdown.high // 0),
              input_tokens:    ($response.token_usage.input_tokens // 0),
              output_tokens:   ($response.token_usage.output_tokens // 0),
              total_tokens:    ($response.token_usage.total_tokens // 0),
              latency_ms:      ($response.latency_ms // 0),
              model:           ($response.model // ""),
              cost_usd:        ($response.cost_usd // 0)
            }')

          curl -s -o /dev/null -w "Metrics ingest: HTTP %{http_code}\n" \
            --max-time 30 \
            -X POST "${API_URL}/api/metrics/ingest" \
            -H "Content-Type: application/json" \
            -H "x-api-key: ${API_KEY}" \
            -d "$METRIC_PAYLOAD" || echo "Warning: summary metrics ingest failed (non-blocking)"

      # ‚îÄ‚îÄ Step 8: Emit full drill-down detail to observability backend ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Report drill-down detail
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true' && always()
        env:
          API_URL: ${{ secrets.AI_AGENT_API_URL }}
          API_KEY: ${{ secrets.AI_AGENT_API_KEY }}
          REPO: ${{ github.repository }}
          PROJECT_ID: ${{ github.event.repository.name }}
          WORKFLOW_RUN_ID: ${{ github.run_id }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          COMMIT_SHA: ${{ github.sha }}
          BRANCH: ${{ github.head_ref }}
          BASE_BRANCH: ${{ github.base_ref }}
          ACTOR: ${{ github.actor }}
        run: |
          if [ ! -f /tmp/review_response.json ]; then
            echo "No review response ‚Äî skipping drill-down detail."
            exit 0
          fi

          R=$(cat /tmp/review_response.json)

          DETAIL_PAYLOAD=$(jq -n \
            --arg  repo          "$REPO" \
            --arg  project_id    "$PROJECT_ID" \
            --arg  run_id        "$WORKFLOW_RUN_ID" \
            --argjson pr_number  "${PR_NUMBER:-0}" \
            --arg  commit_sha    "$COMMIT_SHA" \
            --arg  branch        "$BRANCH" \
            --arg  base_branch   "$BASE_BRANCH" \
            --arg  actor         "$ACTOR" \
            --argjson response   "$R" \
            '{
              run_id:               $response.run_id,
              repository:           $repo,
              project_id:           $project_id,
              workflow_run_id:      $run_id,
              pr_number:            $pr_number,
              commit_sha:           $commit_sha,
              branch:               $branch,
              base_branch:          $base_branch,
              actor:                $actor,
              source:               "pipeline",
              gate_status:          ($response.gate_status // "pass"),
              status:               ($response.status // "success"),
              validation_failures:  ($response.validation_failures // []),
              llm_findings:         ($response.llm_findings // []),
              per_file_results:     ($response.per_file_results // []),
              severity_distribution: ($response.severity_breakdown // {}),
              total_issues:         ($response.total_issues // 0),
              critical_count:       ($response.severity_breakdown.critical // 0),
              high_count:           ($response.severity_breakdown.high // 0),
              medium_count:         ($response.severity_breakdown.medium // 0),
              low_count:            ($response.severity_breakdown.low // 0),
              files_reviewed:       ($response.files_reviewed // 0),
              input_tokens:         ($response.token_usage.input_tokens // 0),
              output_tokens:        ($response.token_usage.output_tokens // 0),
              total_tokens:         ($response.token_usage.total_tokens // 0),
              cost_usd:             ($response.cost_usd // 0),
              model:                ($response.model // ""),
              latency_ms:           ($response.latency_ms // 0),
              timestamp:            ($response.timestamp // ""),
              runtime_ms:           ($response.latency_ms // 0)
            }')

          curl -s -o /dev/null -w "Drill-down detail: HTTP %{http_code}\n" \
            --max-time 30 \
            -X POST "${API_URL}/api/metrics/runs" \
            -H "Content-Type: application/json" \
            -H "x-api-key: ${API_KEY}" \
            -d "$DETAIL_PAYLOAD" || echo "Warning: drill-down detail storage failed (non-blocking)"

      # ‚îÄ‚îÄ Step 9: Enforce gate ‚Äî fail the job if review failed ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
      - name: Enforce PR gate
        if: steps.diff.outputs.no_files == 'false' && steps.payload.outputs.no_files != 'true'
        env:
          GATE_STATUS: ${{ steps.review.outputs.gate_status }}
          CRITICAL: ${{ steps.review.outputs.critical }}
          VAL_FAILURES: ${{ steps.review.outputs.validation_failures }}
        run: |
          if [ "$GATE_STATUS" = "fail" ]; then
            echo "::error title=AI Code Review Gate FAILED::Found ${CRITICAL} critical issue(s) and ${VAL_FAILURES} validation failure(s)."
            echo "::error::Review the PR comment for a full breakdown. Fix all blocking issues before merging."
            exit 1
          fi
          echo "‚úÖ AI Code Review Gate PASSED ‚Äî no blocking issues found."
